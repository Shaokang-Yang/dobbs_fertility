{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71467e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ce5ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_type = \"births\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0507d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alabama']\n",
      "['Alaska']\n",
      "['Arizona']\n",
      "['Arkansas']\n",
      "['California']\n",
      "['Colorado']\n",
      "['Connecticut']\n",
      "['Delaware']\n",
      "['District of Columbia']\n",
      "['Florida']\n",
      "['Georgia']\n",
      "['Hawaii']\n",
      "['Idaho']\n",
      "['Illinois']\n",
      "['Indiana']\n",
      "['Iowa']\n",
      "['Kansas']\n",
      "['Kentucky']\n",
      "['Louisiana']\n",
      "['Maine']\n",
      "['Maryland']\n",
      "['Massachusetts']\n",
      "['Michigan']\n",
      "['Minnesota']\n",
      "['Mississippi']\n",
      "['Missouri']\n",
      "['Montana']\n",
      "['Nebraska']\n",
      "['Nevada']\n",
      "['New Hampshire']\n",
      "['New Jersey']\n",
      "['New Mexico']\n",
      "['New York']\n",
      "['North Carolina']\n",
      "['North Dakota']\n",
      "['Ohio']\n",
      "['Oklahoma']\n",
      "['Oregon']\n",
      "['Pennsylvania']\n",
      "['Rhode Island']\n",
      "['South Carolina']\n",
      "['South Dakota']\n",
      "['Tennessee']\n",
      "['Texas']\n",
      "['Utah']\n",
      "['Vermont']\n",
      "['Virginia']\n",
      "['Washington']\n",
      "['West Virginia']\n",
      "['Wisconsin']\n",
      "['Wyoming']\n",
      "['Alabama', 'Arkansas', 'Georgia', 'Idaho', 'Kentucky', 'Louisiana', 'Mississippi', 'Missouri', 'Oklahoma', 'South Dakota', 'Tennessee', 'Texas', 'West Virginia', 'Wisconsin']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "births_subgroup_definitions = {\n",
    "        'race': (\"nhwhite\", \"hisp\", \"nhblack\", \"otherraceeth\"),\n",
    "        'edu': (\"nohs\", \"hs\", \"somecoll\", \"coll\"),\n",
    "       # 'edu': (\"hs_less\", \"somecoll_more\"),\n",
    "        #'age': (\"age1519\", \"age2024\", \"age2529\", \"age3034\", \"age3539\", \"age4044\"),\n",
    "        'age': (\"age1524\", \"age2534\", \"age3544\"),\n",
    "        'insurance': (\"medicaid\", \"nonmedicaid\"),\n",
    "        # California should be dropped for marital. \n",
    "        'marital': (\"married\", \"unmarried\"),\n",
    "        'total': (\"total\",),\n",
    "    }\n",
    "deaths_subgroup_definitions = {\n",
    "        'race': (\"nhwhite\", \"hisp\", \"nhblack\", \"otherraceeth\"),\n",
    "        'neonatal' : (\"neo\", \"nonneo\"),\n",
    "        'congenital' : (\"con\", \"noncon\"),\n",
    "        'total': (\"total\",),\n",
    "    }\n",
    "\n",
    "subgroup_definitions = {\n",
    "    'births': births_subgroup_definitions,\n",
    "    'deaths': deaths_subgroup_definitions\n",
    "}\n",
    "\n",
    "def clean_dataframe(dat:pd.DataFrame, outcome_type=\"births\", cat_name=\"total\", \n",
    "                    dobbs_donor_sensitivity=False,\n",
    "                    csv_filename='data/dat_quarterly.csv'):\n",
    "    \"\"\"\n",
    "    Filters, imputes, and adds relevant columns to the dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Set time variable for bimonths\n",
    "    dat['time'] = pd.to_datetime(dat.year.astype(str) + '-' + (dat.bmcode * 2 - 1).astype(str) + \"-01\")\n",
    "\n",
    "    # Convert \"births_nhblack\" column to numeric, replacing \"Suppressed\" with NaN\n",
    "    dat['births_nhblack'] = pd.to_numeric(dat['births_nhblack'].replace(\"Suppressed\", pd.NA))\n",
    "\n",
    "    # Create a new column 'partial_ban' based on conditions from 'dobbscodev3' column\n",
    "    # dat['partial_ban'] = dat['dobbscodev3'].apply(lambda x: 1 if x == 2 else 0)\n",
    "    #dat['time'] = pd.to_datetime(dat.year.astype(str) + '-' + dat.month, format=\"%Y-%B\")\n",
    "\n",
    "    def fill_in_missing_denoms(dat):\n",
    "        # Get a list of column names containing the string \"pop\"\n",
    "        cols_with_pop = dat.filter(regex=r'pop').columns\n",
    "        print(dat['state'].unique())\n",
    "        # Iterate over each column containing \"pop\"\n",
    "        for col in cols_with_pop:\n",
    "            # Find the row index of the maximum value for the current column in 2022\n",
    "            pop_index_2022 = dat.loc[dat['year'] == 2022, col].idxmax()\n",
    "            # Find the row index of the maximum value for the current column in 2021\n",
    "            pop_index_2021 = dat.loc[dat['year'] == 2021, col].idxmax()\n",
    "            \n",
    "            # Impute missing values in the current column\n",
    "            # For rows with missing values, replace with the squared value of the 2022 row divided by the 2021 row\n",
    "            dat.loc[dat[col].isna(), col] = (dat.loc[pop_index_2022, col] ** 2) / dat.loc[pop_index_2021, col]\n",
    "        \n",
    "        # Return the modified DataFrame\n",
    "        return dat\n",
    "\n",
    "    # Hacky imputation\n",
    "    # All of 2023 currently has a population of `NA`\n",
    "    # We'll use a linear imputation of (pop 2022) * (pop 2022) / (pop 2021)\n",
    "    # Well do so by:\n",
    "    ## Group the original DataFrame by 'state'\n",
    "    ## Apply the fill_in_missing_denoms function to each group\n",
    "    ## Ungroup the result and reset the index\n",
    "    dat = dat.groupby('state').apply(fill_in_missing_denoms).reset_index(drop=True)\n",
    "\n",
    "    # create a column that is YYYY-QQ for indexing later\n",
    "    #dat = dat.assign(time=dat['year'].astype(str) + '-' + ((dat['q'] - 1) * 3 + 1).astype(str))\n",
    "    dat['quarter'] = dat['time'].apply(lambda x: f\"{pd.Period(x, freq='Q').start_time.year}-Q{pd.Period(x, freq='Q').quarter}\")\n",
    "\n",
    "\n",
    "    ## Correct for different number of days each month\n",
    "\n",
    "    # Assuming 'dat' is a DataFrame in Python\n",
    "    # Convert 'time' column to datetime if it's not already\n",
    "    dat['time'] = pd.to_datetime(dat['time'])\n",
    "\n",
    "    # Filter the DataFrame to keep rows with time before 2024-12-01\n",
    "    dat = dat[dat['time'] < pd.to_datetime('2024-01-01')]\n",
    "\n",
    "    # Calculate the interval in days and divide by 91 to get the multiplier\n",
    "    dat['days_multiplier'] = (dat['time'] + pd.DateOffset(months=3) - dat['time']).dt.days / 91\n",
    "\n",
    "\n",
    "    ## States where abortion was banned in July 2022 include Alabama, Arkansas, Mississippi, Missouri, Oklahoma, South Dakota, \n",
    "    # Texas, West Virginia, and Wisconsin. Kentucky and Louisiana banned abortion in August and Idaho and Tennessee \n",
    "    # did so in September 2022. North Dakota banned abortion in April 2023. Ohio and South Carolina had six-week bans in place \n",
    "    # for the first two months after the Dobbs decision and Georgia had a six-week ban beginning in August 2022.  \n",
    "    # States where abortion was banned in July 2022\n",
    "    # july_banned_states = ['Alabama', 'Arkansas', 'Mississippi', 'Missouri', 'Oklahoma', 'South Dakota', 'Texas', 'West Virginia', 'Wisconsin']\n",
    "\n",
    "    # # States where abortion was banned in August 2022\n",
    "    # august_banned_states = ['Kentucky', 'Louisiana']\n",
    "\n",
    "    # # States where abortion was banned in September 2022\n",
    "    # september_banned_states = ['Idaho', 'Tennessee']\n",
    "\n",
    "    # # States where abortion was banned in April 2023\n",
    "    # april_banned_states = ['North Dakota']\n",
    "\n",
    "    # # States with six-week bans in place for the first two months after the Dobbs decision\n",
    "    # six_week_ban_states = ['Ohio', 'South Carolina']\n",
    "\n",
    "    # # Georgia had a six-week ban beginning in August 2022\n",
    "    # georgia_six_week_ban = ['Georgia']\n",
    "\n",
    "    # # Combine all banned states\n",
    "    # banned_states = (\n",
    "    #     july_banned_states +\n",
    "    #     august_banned_states +\n",
    "    #     september_banned_states +\n",
    "    #     april_banned_states +\n",
    "    #     six_week_ban_states +\n",
    "    #     georgia_six_week_ban\n",
    "    # )\n",
    "\n",
    "\n",
    "    # # Assign dobbscodev2 to dobbs_code\n",
    "    # dat['dobbscodev2'] = dat.groupby('state')['dobbscodev2'].ffill()\n",
    "    \n",
    "    # dat['dobbs_code'] = dat['dobbscodev2']\n",
    "\n",
    "    # # Set dobbs_code to 0 for Texas for times < 2022-04-01\n",
    "    # dat.loc[(dat['state'] == 'Texas') & (dat['time'] < pd.to_datetime('2022-04-01')), 'dobbs_code'] = 0\n",
    "\n",
    "    # # Set dobbs_code to 0 for all other states for times < 2023-01-01\n",
    "    # dat.loc[(dat['state'] != 'Texas') & (dat['time'] < pd.to_datetime('2023-01-01')), 'dobbs_code'] = 0\n",
    "\n",
    "    # # Set dobbs_code to 0 for Kentucky and Louisiana for times before 2023-02-01\n",
    "    # dat.loc[(dat['state'].isin(['Kentucky', 'Louisiana'])) & (dat['time'] < pd.to_datetime('2023-02-01')), 'dobbs_code'] = 0\n",
    "    \n",
    "    # # Set dobbs_code to 0 for Idaho and Tennessee for times before 2023-03-01\n",
    "    # dat.loc[(dat['state'].isin(['Idaho', 'Tennessee'])) & (dat['time'] < pd.to_datetime('2023-03-01')), 'dobbs_code'] = 0\n",
    "\n",
    "    # # Set dobbs_code to 0 for Georgia\n",
    "    # dat.loc[(dat['state'] == 'Georgia'), 'dobbs_code'] = 0\n",
    "    # dat.loc[(dat['state'].isin(['Georgia'])) & (dat['time'] > pd.to_datetime('2023-04-01')), 'dobbs_code'] = 1\n",
    "    \n",
    "\n",
    "\n",
    "        # Create a control index array DataFrame\n",
    "    if outcome_type == \"births\":\n",
    "        dat['exposure_code'] = dat['exposed_births']\n",
    "    if outcome_type == \"deaths\":\n",
    "        dat['exposure_code'] = dat['exposed_infdeaths']\n",
    "    # Convert to a list of unique states with dobbs_code == 1\n",
    "    \n",
    "    states_with_ban = dat.loc[dat['exposure_code'] == 1, 'state'].unique().tolist()\n",
    "    print(states_with_ban)\n",
    "\n",
    "    # Create a new column 'births_other' by subtracting births of non-Hispanic white, Hispanic, and non-Hispanic black from total births\n",
    "    dat['births_other'] = dat['births_total'] - dat['births_nhwhite'] - dat['births_hisp'] - dat['births_nhblack']\n",
    "\n",
    "    # Create a new column 'pop_other' by subtracting population of non-Hispanic white, Hispanic, and non-Hispanic black from total population\n",
    "    dat['pop_other'] = dat['pop_total'] - dat['pop_nhwhite'] - dat['pop_hisp'] - dat['pop_nhblack']\n",
    "    dat = dat.sort_values(['state', 'time'])\n",
    "\n",
    "    # Remove California for marital\n",
    "    if cat_name == \"marital\":\n",
    "        dat = dat[dat[\"state\"] != \"California\"]\n",
    "    \n",
    "    if dobbs_donor_sensitivity:\n",
    "        if \"dobbscode_sensitivity\" in dat.columns:\n",
    "            sensitivity_states = dat[~dat[\"dobbscode_sensitivity\"].isna()]['state'].unique()\n",
    "            sensitivity_states = [state for state in sensitivity_states if state not in ['Arizona', 'Pennsylvania', 'Florida', 'California']]\n",
    "            dat = dat[dat[\"state\"].isin(sensitivity_states)]\n",
    "        else:\n",
    "            print(\"⚠️ Warning: `dobbscode_sensitivity` column not found, skipping donor sensitivity filtering.\")\n",
    "\n",
    "    \n",
    "\n",
    "    if csv_filename is not None:\n",
    "        ## Save to csv so we don't have to do this every time \n",
    "        dat.to_csv(csv_filename)\n",
    "    return dat\n",
    "\n",
    "\n",
    "def prep_data(dat, group=None, outcome_type=\"births\", variables=None, covariates=None):\n",
    "    \"\"\"\n",
    "    Prepare data for analysis by creating DataFrames for births or deaths (numerators), population or births (denominators), control indices, and missing indices.\n",
    "\n",
    "    Args:\n",
    "        dat (pandas.DataFrame): Input data containing information about births, population, and other relevant variables.\n",
    "        variables (list, optional): List of variable names (e.g., \"white\", \"hisp\", \"black\", \"other\"). Default is [\"white\", \"hisp\", \"black\", \"other\"].\n",
    "        covariates (list, optional): List of covariate names to include in the analysis. Default is None.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the following items:\n",
    "            Y (pandas.DataFrame): DataFrame with birth counts for each category, state, and time period.\n",
    "            population (pandas.DataFrame): DataFrame with population counts for each category, state, and time period.\n",
    "            state_fe (numpy.ndarray): Array of state fixed effects.\n",
    "            control_idx_array (pandas.DataFrame): DataFrame with control indices for each category, state, and time period.\n",
    "            missing_idx_array (pandas.DataFrame): DataFrame with missing indices for each category, state, and time period.\n",
    "            days_multiplier (float): Days multiplier value.\n",
    "            variables (list): List of variable names used in the analysis.\n",
    "            D_cov (numpy.ndarray or None): Matrix of covariates, if provided. If no covariates are provided, D_cov is set to None.\n",
    "    \"\"\"\n",
    "    if (group is not None) and (variables is not None):\n",
    "        raise Exception(\"Only one of group/variables can be specified.\")\n",
    "    if group is not None:\n",
    "        variables = subgroup_definitions[outcome_type][group]\n",
    "    \n",
    "\n",
    "    if outcome_type == \"deaths\":\n",
    "        \n",
    "        # Create a list of death column names\n",
    "        death_columns = [\"deaths_\" + var for var in variables]    \n",
    "        \n",
    "        # Create a deaths DataFrame\n",
    "        deaths = (\n",
    "            dat[[\"state\", \"time\"] + death_columns]  # Select 'state', 'time', and death columns\n",
    "            .melt(id_vars=[\"state\", \"time\"], value_vars=death_columns, var_name=\"category\", value_name=\"deaths\")  # Melt death columns into long format\n",
    "            .pivot_table(index=[\"category\"], columns=[\"state\", \"time\"], values=\"deaths\", aggfunc=\"sum\", fill_value=0)  # Pivot to wide format, summing death values\n",
    "        )\n",
    "\n",
    "    # Create a list of birth column names\n",
    "    birth_columns = [\"births_\" + var for var in variables]\n",
    "    \n",
    "    # Create a list of population column names\n",
    "    denom_columns = [\"pop_\" + var for var in variables]\n",
    "\n",
    "    if outcome_type == \"births\":\n",
    "        # Create a population DataFrame\n",
    "        population = (\n",
    "            dat[[\"state\", \"time\"] + denom_columns]  # Select 'state', 'time', and population columns\n",
    "            .melt(id_vars=[\"state\", \"time\"], value_vars=denom_columns, var_name=\"category\", value_name=\"population\")  # Melt population columns into long format\n",
    "            .pivot_table(index=[\"category\"], columns=[\"state\", \"time\"], values=\"population\", aggfunc=\"sum\", fill_value=0)  # Pivot to wide format, summing population values \n",
    "        ) \n",
    "\n",
    "    # Create a births DataFrame\n",
    "    births = (\n",
    "        dat[[\"state\", \"time\"] + birth_columns]  # Select 'state', 'time', and birth columns\n",
    "        .melt(id_vars=[\"state\", \"time\"], value_vars=birth_columns, var_name=\"category\", value_name=\"births\")  # Melt birth columns into long format\n",
    "        .pivot_table(index=[\"category\"], columns=[\"state\", \"time\"], values=\"births\", aggfunc=\"sum\", fill_value=0)  # Pivot to wide format, summing birth values\n",
    "    )\n",
    "    \n",
    "    if outcome_type == \"deaths\":\n",
    "        Y = deaths\n",
    "        denominators = births\n",
    "        outcome_columns = death_columns\n",
    "    else:\n",
    "        Y = births\n",
    "        denominators = population / 1e4 # Population per 1000\n",
    "        outcome_columns = birth_columns\n",
    "    \n",
    "    control_idx_array = (\n",
    "        dat[[\"state\", \"time\", \"exposure_code\"] + outcome_columns]  # Select 'state', 'time', 'exposed_births', and birth/death columns\n",
    "        .melt(id_vars=[\"state\", \"time\", \"exposure_code\"], value_vars=outcome_columns, var_name=\"category\", value_name=outcome_type)  # Melt birth columns into long format\n",
    "        .assign(ctrl_index=(lambda x: x[\"exposure_code\"] == 0))  # Create a control index column based on 'exposed_births'\n",
    "        .pivot_table(index=[\"category\"], columns=[\"state\", \"time\"], values=\"ctrl_index\", aggfunc=\"sum\", fill_value=0)  # Pivot to wide format, summing control index values\n",
    "    ).astype(np.bool_) # cast to a boolean so we don't have issues when we mask\n",
    "    \n",
    "    # Create a missing index array DataFrame\n",
    "    missing_idx_array = (\n",
    "        dat[[\"state\", \"time\", \"exposure_code\"] + outcome_columns]  # Select 'state', 'time', 'exposure_code', and birth columns\n",
    "        .melt(id_vars=[\"state\", \"time\", \"exposure_code\"], value_vars=outcome_columns, var_name=\"category\", value_name=outcome_type)  # Melt birth columns into long format\n",
    "        .assign(missing_index=lambda x: x[outcome_type].isna().astype(int))  # Create a missing index column based on missing birth values\n",
    "        .pivot_table(index=[\"category\"], columns=[\"state\", \"time\"], values=\"missing_index\", aggfunc=\"sum\", fill_value=0)  # Pivot to wide format, summing missing index values\n",
    "    ).astype(np.bool_) # cast to a boolean so we don't have issues when we mask\n",
    "    \n",
    "    num_states = len(dat.state.unique())\n",
    "    total_length = denominators.shape[1]\n",
    "    denominators = denominators.values.reshape((len(variables), num_states, denominators.shape[1]//num_states))\n",
    "    Y = Y.values.reshape((len(variables), num_states, total_length//num_states))\n",
    "\n",
    "    # If covariates are provided, calculate the covariates matrix\n",
    "    if covariates is not None:\n",
    "        D_cov = dat.groupby(\"state\")[covariates].mean().reset_index()[covariates].values\n",
    "        D_cov[np.isnan(D_cov)] = D_cov[~np.isnan(D_cov)].mean()\n",
    "    else:\n",
    "        D_cov = None\n",
    "    \n",
    "\n",
    "    # Return a dictionary with the calculated values\n",
    "    return {\n",
    "        \"Y\": Y,\n",
    "        \"denominators\": denominators,\n",
    "        #\"state_fe\": state_fe,\n",
    "        \"control_idx_array\": control_idx_array.values.reshape((len(variables), num_states, total_length//num_states)),\n",
    "        \"missing_idx_array\": missing_idx_array.values.reshape((len(variables), num_states, total_length//num_states)),\n",
    "        #\"days_multiplier\": days_multiplier,\n",
    "        \"variables\": variables,\n",
    "        \"D_cov\": D_cov,\n",
    "    }\n",
    "\n",
    "def create_unit_placebo_dataset(df, treated_state = \"Texas\", placebo_state = \"California\"):\n",
    "    \"\"\"\n",
    "    Create a placebo dataset for by giving Texas' treatment times to `placebo_state` state by removing the state \n",
    "    and removing Texas.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input data containing information about births, population, exposure codes and other relevant variables.\n",
    "        placebo_state (str): Name of the placebo state to remove from the dataset.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Placebo dataset with the specified placebo state with treatment times of treated_state.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Creating unit-placebo dataset for {}\".format(placebo_state))\n",
    "\n",
    "    # Get the columns that start with 'exposed' (exposed_births/exposed_deaths)\n",
    "    exposure_columns = df.filter(regex='^exposure').columns\n",
    "\n",
    "    # Get the values from the rows where 'state' equals 'treated_state'\n",
    "    treated_values = df.loc[df['state'] == treated_state, exposure_columns]\n",
    "\n",
    "    # Set the values in the rows where 'state' equals 'placebo_state' to the values from the 'treated_state' rows\n",
    "    df.loc[df['state'] == placebo_state, exposure_columns] = treated_values.values\n",
    "    \n",
    "    # Filter the DataFrame to keep rows where 'state' is not equal to the treated state\n",
    "    return df[df[\"state\"] != treated_state]\n",
    "\n",
    "def create_time_placebo_dataset(df, new_treatment_start=\"2022-05-01\"):\n",
    "    \"\"\"\n",
    "    Create a time placebo dataset by shifting treatment times early and capping the end date.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input data containing information about births, population, exposure codes and other relevant variables.\n",
    "        first_treatment_start (str): Start time of the first treated unit (usually Texas). Set to \"2022-05-01\" by default which is the actual SB8 time.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Modified dataset with shifted time variables.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Creating placebo-in-time dataset starting in {}\".format(new_treatment_start))\n",
    "    \n",
    "    # Convert 'first_treatment_start' to datetime\n",
    "        \n",
    "    new_treatment_start = pd.to_datetime(new_treatment_start)\n",
    "    original_treatment_start = df.loc[df['exposure_code'] == 1, 'time'].min()\n",
    "\n",
    "    end_date = df.loc[df['exposure_code'] == 1, 'time'].max()\n",
    "    \n",
    "    # add 1 week to avoid rounding months down when date is the last day of month\n",
    "    new_end = new_treatment_start + (end_date - original_treatment_start) + timedelta(days=7) \n",
    "    \n",
    "    original_time_length = end_date - pd.to_datetime(\"2016-01-01\")\n",
    "    new_start = new_end - original_time_length\n",
    "    if new_start < df[\"time\"].min():\n",
    "        new_start = df[\"time\"].min() + timedelta(days=7) # add 1 week to avoid rounding down\n",
    "    \n",
    "    new_start = new_start.to_period('M').to_timestamp()\n",
    "    new_end = new_end.to_period('M').to_timestamp()\n",
    "    \n",
    "    new_time_length = new_end - new_start\n",
    "\n",
    "    # Get the columns that start with 'exposure_code'\n",
    "\n",
    "    exposure_code_values = df.loc[df['time'] >= (end_date - new_time_length + timedelta(days=7)).to_period('M').to_timestamp(), ['exposure_code']]\n",
    "\n",
    "    df = df[(df['time'] <= new_end) & (df['time'] >= new_start)]\n",
    "\n",
    "    print(exposure_code_values.values.shape)\n",
    "    print(df.shape)\n",
    "    if len(exposure_code_values) == len(df):\n",
    "        df.loc[:, \"exposure_code\"] = exposure_code_values.values\n",
    "    else:\n",
    "        raise ValueError(\"The length of new exposure_code values does not match the number of rows in df\")\n",
    "\n",
    "    return df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "    import pandas as pd\n",
    "    import pickle \n",
    "    import os\n",
    "    import gzip\n",
    "    import sys\n",
    "    sys.argv = ['script.py', '/Users/shaokangyang/Downloads/df_through_june.csv', '--save-dict', '--group', 'race']\n",
    " \n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Script for cleaning and parsing quarterly births. We will assume that the file name is in the format quarterly_fertility_mortality_MMDDYY.csv where MM is month,DD is day, and YY is year.')\n",
    "    parser.add_argument('filename', type=str, help='Name of the input file')\n",
    "    parser.add_argument('--save-dict', action='store_true', help='Create and save the dictionary used for model training as a pkl file')\n",
    "    parser.add_argument(\"--group\", help=\"Subgroups to create a dictionary for\", default=\"total\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    # Create a directory to hold the parsed files\n",
    "    directory_name = '/Users/shaokangyang/Downloads/model_data/' + args.filename.split('/')[-1].split('.')[0]\n",
    "    os.makedirs(directory_name, exist_ok=True)\n",
    "    dat = clean_dataframe(pd.read_csv(args.filename))\n",
    "    # Save the dataframe\n",
    "    dat.to_csv(directory_name + '/data_frame.csv')\n",
    "\n",
    "    if args.save_dict:\n",
    "        # data_dict = prep_data(dat, subgroup_definitions[outcome_type][args.group])\n",
    "        data_dict = prep_data(dat, group=args.group, outcome_type=\"births\")\n",
    "\n",
    "        # Specify the filename for the gzipped pickle file\n",
    "        dict_filename = 'births_{}_dict.pkl.gz'.format(args.group)\n",
    "\n",
    "        # Open the file in binary write mode using gzip\n",
    "        with gzip.open('{}/{}'.format(directory_name, dict_filename), 'wb') as f:\n",
    "            # Use pickle to dump the data dictionary into the file\n",
    "            pickle.dump(data_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
